{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import xml.etree.ElementTree as ET\n",
    "from copy import deepcopy\n",
    "import random\n",
    "\n",
    "from moviepy.editor import VideoFileClip,ImageSequenceClip\n",
    "from IPython.display import HTML\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from matplotlib.figure import Figure\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "import pickle\n",
    "from pylab import *\n",
    "from matplotlib import gridspec\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "cdict = {'red': ((0.0, 1.0, 1.0),\n",
    "                 (0.125, 1.0, 1.0),\n",
    "                 (0.25, 1.0, 1.0),\n",
    "                 (0.5625, 1.0, 1.0),\n",
    "                 (1.0, 0.0, 0.0)),\n",
    "         'green': ((0.0, 0.0, 0.0),\n",
    "                   (0.25, 0.0, 0.0),\n",
    "                   (0.5625, 1.0, 1.0),\n",
    "                   (1.0, 1.0, 1.0)),\n",
    "         'blue': ((0.0, 0.0, 0.0),\n",
    "                  (0.5, 0.0, 0.0),\n",
    "                  (1.0, 0.0, 0.0))}\n",
    "my_cmap = matplotlib.colors.LinearSegmentedColormap('my_colormap',cdict,256)\n",
    "\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "%matplotlib inline\n",
    "# %matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_history = True\n",
    "weights_file_name = 'CNN_weather_and_history'\n",
    "\n",
    "# use_history = False\n",
    "# weights_file_name = 'CNN_weather_only'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "which_data = 'Data_2015_DES_I235E'\n",
    "(Traffic_2015,Weather_2015,data_2015) = pickle.load( open(which_data+\"_filter_smooth.p\", \"rb\" ) )\n",
    "year =  data_2015['y'][0][:4]\n",
    "print('{} data loaded'.format(year))\n",
    "\n",
    "which_data = 'Data_2016_DES_I235E'\n",
    "(Traffic_2016,Weather_2016,data_2016) = pickle.load( open(which_data+\"_filter_smooth.p\", \"rb\" ) )\n",
    "year =  data_2016['y'][0][:4]\n",
    "print('{} data loaded'.format(year))\n",
    "\n",
    "data = pd.concat([data_2015,data_2016],ignore_index=True)\n",
    "Traffic = np.concatenate([Traffic_2015, Traffic_2016],axis = 0)\n",
    "Weather_5min = np.concatenate([Weather_2015, Weather_2016],axis = 0)\n",
    "Weather = np.zeros([Weather_5min.shape[0],Weather_5min.shape[1],Weather_5min.shape[2]*5,Weather_5min.shape[3]])\n",
    "for i in range(Weather.shape[2]):\n",
    "    Weather[:,:,i,:] = Weather_5min[:,:,int(i/5),:]\n",
    "Weather = np.delete(Weather,1,axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_SpeedPrediction_database(Dataset):\n",
    "    def __init__(self, Traffic, Weather, data, transform=None, look_back = 6):\n",
    "\n",
    "        self.Traffic = Traffic\n",
    "        self.Weather = Weather\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        self.look_back = look_back\n",
    "#         printdb_trans_1 = Dataset_NVIDIA_1(annotation_list,frame_list,transform = PerspectiveTransform(MAP_FILE, world_origin, pixel_ratio, PMAT_FILE))(self.__len__())\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        assert idx < self.__len__(),'idx out of dataset index boundary'\n",
    "        Traffic_today = self.Traffic[idx,:,:,:]\n",
    "        Weather_today = self.Weather[idx,:,:,:]\n",
    "        Traffic_history = 0 \n",
    "        Traffic_output = self.Traffic[idx,:,:,0]\n",
    "        \n",
    "        dayofweek = self.data.loc[idx]['dayofweek']\n",
    "        data_sub = self.data[:idx]\n",
    "        sameday_in_history = data_sub.index[data_sub['dayofweek'] == dayofweek].tolist()\n",
    "        \n",
    "#         Traffic_history = np.full((self.look_back,)+self.Traffic.shape[1:], np.nan)\n",
    "        \n",
    "        Traffic_history = [self.Traffic[idx,:,:,:]+np.random.randn(15,1440,3) for _ in range(self.look_back)]\n",
    "        Traffic_history = np.stack(Traffic_history,0)\n",
    "        \n",
    "#         print(len(sameday_in_history))\n",
    "        sameday_in_near_history = sameday_in_history[-self.look_back:]\n",
    "#         print(len(sameday_in_near_history))\n",
    "        \n",
    "        for i in range(len(sameday_in_near_history)):\n",
    "            Traffic_history[self.look_back-len(sameday_in_near_history)+i] = self.Traffic[sameday_in_near_history[i]]\n",
    "        \n",
    "        sample = {'weather': torch.Tensor(Weather_today), 'history': torch.Tensor(Traffic_history), 'label': torch.Tensor(Traffic_output)} # \n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = CNN_SpeedPrediction_database(Traffic, Weather, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 431  # test: [417,420,431]\n",
    "print(data.loc[idx])\n",
    "\n",
    "def vis_one_day_traffic_speed(speed):\n",
    "    plt.pcolor(speed,cmap=my_cmap, vmin=20, vmax=70)\n",
    "    \n",
    "# vis_one_day_traffic_speed(db[257]['label'])\n",
    "print(db[idx].keys())\n",
    "print(db[idx]['history'].shape)\n",
    "print(db[idx]['weather'].shape)\n",
    "print(db[idx]['label'].shape)\n",
    "vis_one_day_traffic_speed(db[idx]['label']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data[:-30]\n",
    "data_train = data_train.reset_index()\n",
    "\n",
    "data_val = data[-61:]\n",
    "data_val = data_val.reset_index()\n",
    "\n",
    "dataset = {}\n",
    "dataset['train'] = CNN_SpeedPrediction_database(Traffic[:-30], Weather[:-30], data_train)\n",
    "dataset['val'] = CNN_SpeedPrediction_database(Traffic[-61:], Weather[-61:], data_val)\n",
    "\n",
    "dataset_sizes = {x: len(dataset[x]) for x in ['train', 'val']}\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(dataset[x], batch_size=1000,\n",
    "                                             shuffle=True, num_workers=4)\n",
    "              for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''https://github.com/wkelongws/iemgrid'''\n",
    "'''Weather: 0:tmpc 1:dwpc 2:smps 3:drct 4:vsby 5:roadtmpc 6:srad 7:snwd 8:pcpn '''\n",
    "\n",
    "Traffic_max = []\n",
    "Traffic_min = []\n",
    "Weather_max = []\n",
    "Weather_min = []\n",
    "\n",
    "for i in range(Traffic.shape[-1]):\n",
    "#     print(i,'max: ',np.max(Traffic[:,:,:,i]))\n",
    "    Traffic_max.append(np.max(Traffic[:,:,:,i]))\n",
    "#     print(i,'mean: ',np.mean(Traffic[:,:,:,i]))\n",
    "#     print(i,'min: ',np.min(Traffic[:,:,:,i]))\n",
    "    Traffic_min.append(np.min(Traffic[:,:,:,i]))\n",
    "for i in range(Weather.shape[-1]):\n",
    "#     print(i,'max: ',np.max(Weather[:,:,:,i]))\n",
    "    Weather_max.append(np.max(Weather[:,:,:,i]))\n",
    "#     print(i,'mean: ',np.mean(Weather[:,:,:,i]))\n",
    "#     print(i,'min: ',np.min(Weather[:,:,:,i]))\n",
    "    Weather_min.append(np.min(Weather[:,:,:,i]))\n",
    "    \n",
    "Min_Max = (Traffic_max,Traffic_min,Weather_max,Weather_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for sample in dataloaders['train']:\n",
    "    cnt += 1\n",
    "    if cnt == 1:\n",
    "        s = sample\n",
    "    c = sample['weather']\n",
    "    d = sample['history']\n",
    "    e = sample['label']\n",
    "    print('weather: ',sample['weather'].shape)\n",
    "    print('history: ',sample['history'].shape)\n",
    "    print('label: ',sample['label'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Long_Term_Speed_Pred_Net(nn.Module):\n",
    "    def __init__(self, use_gpu = True, use_history = True, Min_Max = Min_Max):\n",
    "        super(CNN_Long_Term_Speed_Pred_Net, self).__init__()\n",
    "        \n",
    "        self.use_gpu = use_gpu\n",
    "        self.use_history = use_history\n",
    "\n",
    "        '''for on-fly normalization'''\n",
    "        self.Traffic_max = Min_Max[0]\n",
    "        self.Traffic_min = Min_Max[1]\n",
    "        self.Weather_max = Min_Max[2]\n",
    "        self.Weather_min = Min_Max[3]\n",
    "        \n",
    "        self.weather_layer1_conv2d = nn.Conv2d(in_channels=9, out_channels=2, kernel_size=(1,1))\n",
    "        self.weather_layer2_conv2d = nn.Conv2d(in_channels=2, out_channels=2, kernel_size=(3,3))\n",
    "        self.weather_layer3_conv2d = nn.Conv2d(in_channels=2, out_channels=3, kernel_size=(3,3))\n",
    "        \n",
    "        self.history_layer1_conv3d = nn.Conv3d(in_channels=1, out_channels=2, kernel_size=(3,3,3))\n",
    "        self.history_layer2_conv2d = nn.Conv2d(in_channels=8, out_channels=2, kernel_size=(1,1))\n",
    "        self.history_layer3_conv2d = nn.Conv2d(in_channels=2, out_channels=3, kernel_size=(3,3))\n",
    "        \n",
    "        self.decoder_layer1_conv2d = nn.ConvTranspose2d(in_channels=3, out_channels=1, kernel_size=(3,3))\n",
    "        self.decoder_layer2_conv2d = nn.ConvTranspose2d(in_channels=1, out_channels=1, kernel_size=(3,3))\n",
    "\n",
    "    def forward(self, input_sample, future = 0, use_gpu = True):\n",
    "        \n",
    "        '''reconstruct data from sample'''\n",
    "        weather = input_sample['weather']\n",
    "        history = input_sample['history']\n",
    "        label = input_sample['label']\n",
    "        \n",
    "        '''normalize'''\n",
    "#         label = (label - self.Traffic_min[0])/(self.Traffic_max[0]-self.Traffic_min[0])\n",
    "        for i in range(history.shape[-1]):\n",
    "            history[:,:,:,:,i] = (history[:,:,:,:,i]-self.Traffic_min[i])/(self.Traffic_max[i]-self.Traffic_min[i])\n",
    "        for i in range(weather.shape[-1]):\n",
    "            weather[:,:,:,i] = (weather[:,:,:,i]-self.Weather_min[i])/(self.Weather_max[i]-self.Weather_min[i])\n",
    "        \n",
    "        '''re-arrange inputs'''\n",
    "        input_history = history[:,:,:,:,0].unsqueeze(1)\n",
    "        input_weather = weather.permute(0,3,1,2)\n",
    "            \n",
    "        if use_gpu:\n",
    "            input_history = Variable(input_history.cuda())\n",
    "            input_weather = Variable(input_weather.cuda())\n",
    "            label = Variable(label.cuda())\n",
    "        else:\n",
    "            input_history = Variable(input_history)\n",
    "            input_weather = Variable(input_weather)\n",
    "            label = Variable(label)\n",
    "        \n",
    "        '''feedforward'''\n",
    "        weather_feature_map_1 = self.weather_layer1_conv2d(input_weather)\n",
    "        weather_feature_map_2 = self.weather_layer2_conv2d(weather_feature_map_1)\n",
    "        weather_feature_map_3 = self.weather_layer3_conv2d(weather_feature_map_2)\n",
    "        encoded_feature_map = weather_feature_map_3\n",
    "        \n",
    "        if self.use_history:\n",
    "            history_feature_map_1 = self.history_layer1_conv3d(input_history)\n",
    "            history_feature_map_1 = torch.cat([history_feature_map_1[:,i,:,:,:] for i in range(history_feature_map_1.shape[1])],1)\n",
    "            history_feature_map_2 = self.history_layer2_conv2d(history_feature_map_1)\n",
    "            history_feature_map_3 = self.history_layer3_conv2d(history_feature_map_2)\n",
    "            encoded_feature_map += history_feature_map_3\n",
    "        \n",
    "        upsampled_feature_map = self.decoder_layer1_conv2d(encoded_feature_map)\n",
    "        output = self.decoder_layer2_conv2d(upsampled_feature_map).squeeze(1)\n",
    "        output = output*(self.Traffic_max[0]-self.Traffic_min[0])+self.Traffic_min[0] \n",
    "        \n",
    "        return output, label\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "# use_gpu = False\n",
    "\n",
    "model = CNN_Long_Term_Speed_Pred_Net(use_gpu = use_gpu, use_history = use_history, Min_Max = Min_Max)\n",
    "\n",
    "if use_gpu:\n",
    "    model = model.cuda()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_CNN_LongTerm_SP_Net(model,dataloaders, criterion, optimizer, dataset_sizes, num_epochs=100, weights_file_name = weights_file_name):\n",
    "    since = time.time()\n",
    "\n",
    "#     timeSince(since)\n",
    "#     model.load_state_dict(torch.load('Best_LSTM_Weights_1st'))\n",
    "#     print()\n",
    "#     print('keep training from previous \"Best_LSTM_Weights_1st\"')\n",
    "#     print()\n",
    "\n",
    "#     vanillaPlus_compromised_dict = torch.load('Best_LSTM_Weights_vanillaPlus')\n",
    "#     vanillaPlus_compromised_dict['linear_out.weight'] = torch.randn(model.state_dict()['linear_out.weight'].shape).cuda()\n",
    "#     vanillaPlus_compromised_dict['linear_out.bias'] = torch.randn(model.state_dict()['linear_out.bias'].shape).cuda()\n",
    "#     model.load_state_dict(vanillaPlus_compromised_dict)\n",
    "#     print()\n",
    "#     print('weights in the output layer are initialized from normal distribution')\n",
    "#     print('and other model weights are loaded from \"Best_LSTM_Weights_vanillaPlus_compromised')\n",
    "#     print('keep training from previous \"Best_LSTM_Weights_vanillaPlus_compromised\"')\n",
    "#     print()\n",
    "\n",
    "    best_model_wts = model.state_dict()\n",
    "    best_loss = 100000\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train(True)  # Set model to training mode\n",
    "            else:\n",
    "                model.train(False)  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for sample in dataloaders[phase]:\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                model.zero_grad()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Also, we need to clear out the hidden state of the LSTM,\n",
    "                # detaching it from its history on the last instance.\n",
    "#                 model.hidden = model.init_hidden()\n",
    "                \n",
    "                # forward\n",
    "                pred,label = model(sample)\n",
    "                \n",
    "                # loss function\n",
    "#                 loss = customized_loss(pred,label)\n",
    "                loss = criterion(pred,label)\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.data[0]\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} total elapsed time: {}'.format(phase, epoch_loss, timeSince(since)))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = model.state_dict()\n",
    "                torch.save(best_model_wts, weights_file_name)\n",
    "                print('Weights saved to {}'.format(weights_file_name))\n",
    "                print()\n",
    "                pickle.dump( losses, open( 'loss_log_'+weights_file_name+'_.p', \"wb\" ) )\n",
    "\n",
    "        print()\n",
    "\n",
    "    print('Training complete in {}'.format(timeSince(since)))\n",
    "    print('Best val loss: {:4f}'.format(best_loss))\n",
    "    \n",
    "    # load best model weights\n",
    "#     model.load_state_dict(best_model_wts)\n",
    "#     torch.save(model, 'Model_with_Best_CNN_Weights')\n",
    "#     print('Model saved to \"Model_with_Best_CNN_Weights\"')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = train_CNN_LongTerm_SP_Net(model,dataloaders, criterion, optimizer, dataset_sizes, num_epochs=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
