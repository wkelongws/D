{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all the needed libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import xml.etree.ElementTree as ET\n",
    "from copy import deepcopy\n",
    "import random\n",
    "\n",
    "from moviepy.editor import VideoFileClip,ImageSequenceClip\n",
    "from IPython.display import HTML\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from matplotlib.figure import Figure\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "import pickle\n",
    "from pylab import *\n",
    "from matplotlib import gridspec\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "cdict = {'red': ((0.0, 1.0, 1.0),\n",
    "                 (0.125, 1.0, 1.0),\n",
    "                 (0.25, 1.0, 1.0),\n",
    "                 (0.5625, 1.0, 1.0),\n",
    "                 (1.0, 0.0, 0.0)),\n",
    "         'green': ((0.0, 0.0, 0.0),\n",
    "                   (0.25, 0.0, 0.0),\n",
    "                   (0.5625, 1.0, 1.0),\n",
    "                   (1.0, 1.0, 1.0)),\n",
    "         'blue': ((0.0, 0.0, 0.0),\n",
    "                  (0.5, 0.0, 0.0),\n",
    "                  (1.0, 0.0, 0.0))}\n",
    "my_cmap = matplotlib.colors.LinearSegmentedColormap('my_colormap',cdict,256)\n",
    "\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "%matplotlib inline\n",
    "# %matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import, filter and smooth raw data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "which_data = 'Data_2015_DES_I235E'\n",
    "(Traffic_2015,Weather_2015,data_2015) = pickle.load( open(which_data+\".p\", \"rb\" ) )\n",
    "year =  data_2015['y'][0][:4]\n",
    "print('load {} data'.format(year))\n",
    "\n",
    "which_data = 'Data_2016_DES_I235E'\n",
    "(Traffic_2016,Weather_2016,data_2016) = pickle.load( open(which_data+\".p\", \"rb\" ) )\n",
    "year =  data_2016['y'][0][:4]\n",
    "print('load {} data'.format(year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_nan_with_local_mean(M_3d):\n",
    "    index = np.asarray(np.where(np.isnan(M_3d)))\n",
    "    for i in range(index.shape[1]):\n",
    "        span = 0\n",
    "        historical = M_3d[index[0,i],index[1,i],index[2,i]-span:index[2,i]+1+span]\n",
    "        historical_mean = np.nanmean(historical)\n",
    "        stage_1_flag = True\n",
    "        count = 0 \n",
    "        while (not historical_mean>0) & stage_1_flag:\n",
    "            count+=1\n",
    "            if count > 1000:\n",
    "                print('1st ',count)\n",
    "            if index[0,i]-span<0:\n",
    "                historical = M_3d[0:index[0,i]+1+span+span,index[1,i],index[2,i]]\n",
    "            elif index[0,i]+1+span>M_3d.shape[0]:\n",
    "                historical = M_3d[-1-span-span:,index[1,i],index[2,i]]\n",
    "            else:\n",
    "                historical = M_3d[index[0,i]-span:index[0,i]+1+span,index[1,i],index[2,i]]\n",
    "            historical_mean = np.nanmean(historical)\n",
    "            span+=1\n",
    "            if 2*span>M_3d.shape[0]:\n",
    "                stage_1_flag = False\n",
    "        span = 0\n",
    "        count = 0 \n",
    "        while not historical_mean>0:\n",
    "            count+=1\n",
    "            if count > 1000:\n",
    "                print('2nd ',count)\n",
    "            if index[2,i]-span<0:\n",
    "                historical = M_3d[:,index[1,i],0:index[2,i]+1+span+span]\n",
    "            elif index[2,i]+1+span>M_3d.shape[2]:\n",
    "                historical = M_3d[:,index[1,i],-1-span-span:]\n",
    "            else:\n",
    "                historical = M_3d[:,index[1,i],index[2,i]-span:index[2,i]+1+span]\n",
    "            historical_mean = np.nanmean(historical)\n",
    "            span+=1\n",
    "#         print(historical_mean)\n",
    "\n",
    "        span = 0\n",
    "        count = 0 \n",
    "        local = M_3d[index[0,i],index[1,i],index[2,i]-span:index[2,i]+1+span]\n",
    "        local_mean = np.nanmean(local)\n",
    "        stage_1_flag = True\n",
    "        while (not local_mean>0) & stage_1_flag:\n",
    "            count+=1\n",
    "            if count > 1000:\n",
    "                print('3rd ',count)\n",
    "            if index[2,i]-span<0:\n",
    "                local = M_3d[index[0,i],index[1,i],0:index[2,i]+1+span+span]\n",
    "            elif index[2,i]+1+span>M_3d.shape[2]:\n",
    "                local = M_3d[index[0,i],index[1,i],-1-span-span:]\n",
    "            else:\n",
    "                local = M_3d[index[0,i],index[1,i],index[2,i]-span:index[2,i]+1+span]\n",
    "            local_mean = np.nanmean(local)\n",
    "            span+=1\n",
    "            if 2*span>M_3d.shape[2]:\n",
    "                stage_1_flag = False\n",
    "        \n",
    "        span = 0\n",
    "        count = 0 \n",
    "        while not local_mean>0:\n",
    "            count+=1\n",
    "            if count > 1000:\n",
    "                print('4th ',count)\n",
    "            if index[1,i]-span<0:\n",
    "                local = M_3d[index[0,i],0:index[1,i]+1+span+span,:]\n",
    "            elif index[1,i]+1+span>M_3d.shape[1]:\n",
    "                local = M_3d[index[0,i],-1-span-span:,:]\n",
    "            else:\n",
    "                local = M_3d[index[0,i],index[1,i]-span:index[1,i]+1+span,:]\n",
    "            local_mean = np.nanmean(local)\n",
    "            span+=1\n",
    "        \n",
    "#         print(local_mean)\n",
    "        M_3d[index[0,i],index[1,i],index[2,i]] = 0.7*local_mean + 0.3*historical_mean\n",
    "    return M_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def volume_smoothing(Volume,top_threshold = 400):\n",
    "    Volume[Volume>top_threshold] = nan\n",
    "    Volume = replace_nan_with_local_mean(Volume)\n",
    "    return Volume\n",
    "\n",
    "def speed_smoothing(Speed,top_threshold = 120):\n",
    "    Speed[Speed>top_threshold] = nan\n",
    "    Speed = replace_nan_with_local_mean(Speed)\n",
    "    Speed[Speed == 0] = nan\n",
    "    Speed = replace_nan_with_local_mean(Speed)\n",
    "    return Speed\n",
    "\n",
    "print('Filter traffic data .....')\n",
    "# Traffic_2015[:,:,:,0] = speed_smoothing(Traffic_2015[:,:,:,0])\n",
    "# Traffic_2015[:,:,:,1] = volume_smoothing(Traffic_2015[:,:,:,1])\n",
    "\n",
    "# Traffic_2016[:,:,:,0] = speed_smoothing(Traffic_2016[:,:,:,0])\n",
    "# Traffic_2016[:,:,:,1] = volume_smoothing(Traffic_2016[:,:,:,1])\n",
    "print('Done')\n",
    "\n",
    "def moving_avg_batch(data,window_length=5):\n",
    "    data_new = np.zeros(data.shape)\n",
    "    for i in range(window_length):\n",
    "        data_shift = np.copy(data)\n",
    "        data_shift[:,:,i:]=data[:,:,:data.shape[2]-i]\n",
    "        data_new += data_shift\n",
    "    return data_new/window_length\n",
    "\n",
    "def data_smoothing_moving_avg(data,window_length=5):\n",
    "    data_new = np.copy(data)\n",
    "    if len(data.shape)==4:\n",
    "        for i in range(data.shape[-1]):\n",
    "            data_new[:,:,:,i] = moving_avg_batch(data[:,:,:,i],window_length=window_length)\n",
    "    if len(data.shape)==3:\n",
    "        data_new = moving_avg_batch(data,window_length=window_length)\n",
    "    return data_new\n",
    "\n",
    "print('Smooth traffic data .....')\n",
    "# Traffic_2015 = data_smoothing_moving_avg(Traffic_2015)\n",
    "# Traffic_2016 = data_smoothing_moving_avg(Traffic_2016)\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump( (Traffic_2015,Weather_2015,data_2015), open( 'Data_2015_DES_I235E'+'_filter_smooth.p', \"wb\" ) )\n",
    "# pickle.dump( (Traffic_2016,Weather_2016,data_2016), open( 'Data_2016_DES_I235E'+'_filter_smooth.p', \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import saved smoothed data (start from here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "which_data = 'Data_2015_DES_I235E'\n",
    "(Traffic_2015,Weather_2015,data_2015) = pickle.load( open(which_data+\"_filter_smooth.p\", \"rb\" ) )\n",
    "year =  data_2015['y'][0][:4]\n",
    "print('{} data loaded'.format(year))\n",
    "\n",
    "which_data = 'Data_2016_DES_I235E'\n",
    "(Traffic_2016,Weather_2016,data_2016) = pickle.load( open(which_data+\"_filter_smooth.p\", \"rb\" ) )\n",
    "year =  data_2016['y'][0][:4]\n",
    "print('{} data loaded'.format(year))\n",
    "\n",
    "data = pd.concat([data_2015,data_2016],ignore_index=True)\n",
    "Traffic = np.concatenate([Traffic_2015, Traffic_2016],axis = 0)\n",
    "Weather_5min = np.concatenate([Weather_2015, Weather_2016],axis = 0)\n",
    "Weather = np.zeros([Weather_5min.shape[0],Weather_5min.shape[1],Weather_5min.shape[2]*5,Weather_5min.shape[3]])\n",
    "for i in range(Weather.shape[2]):\n",
    "    Weather[:,:,i,:] = Weather_5min[:,:,int(i/5),:]\n",
    "Weather = np.delete(Weather,1,axis=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Data Base\n",
    "* ### Multi-output: each sample includes {'traffic':(15,1439,3),'weather':(15,1439,10),'history':(6,15,1439,3),'label':(15,1439)}\n",
    "* ### Single-output: each sample includes {'traffic':(15,1439,3),'weather':(15,1439,10),'history':(6,15,1439,3),'label':(1439,)}\n",
    "* ### note: in 'history', non-existing days in database are represented in nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_SpeedPrediction_database_output_single_sensor(Dataset):\n",
    "    def __init__(self, Traffic, Weather, data, transform=None, look_back = 6, target_sensor = 13):\n",
    "\n",
    "        self.Traffic = Traffic\n",
    "        self.Weather = Weather\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        self.look_back = look_back\n",
    "        self.target_sensor = target_sensor\n",
    "#         print(self.__len__())\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        assert idx < self.__len__(),'idx out of dataset index boundary'\n",
    "        \n",
    "        Traffic_today = self.Traffic[idx,:,:-1,:]\n",
    "        Weather_today = self.Weather[idx,:,:-1,:]\n",
    "        Traffic_history = 0\n",
    "        Traffic_output = self.Traffic[idx,self.target_sensor,1:,0]\n",
    "        \n",
    "        dayofweek = self.data.loc[idx]['dayofweek']\n",
    "        data_sub = self.data[:idx]\n",
    "        sameday_in_history = data_sub.index[data_sub['dlen(sameday_in_history)ayofweek'] == dayofweek].tolist()\n",
    "        \n",
    "        Traffic_history = np.full((self.look_back,)+self.Traffic.shape[1:], np.nan)\n",
    "#         print(len(sameday_in_history))\n",
    "        sameday_in_near_history = sameday_in_history[-self.look_back:]\n",
    "#         print(len(sameday_in_near_history))\n",
    "        \n",
    "        for i in range(len(sameday_in_near_history)):\n",
    "            Traffic_history[i] = self.Traffic[sameday_in_near_history[i]]\n",
    "        \n",
    "        sample = {'traffic': Traffic_today, 'weather': Weather_today, 'history': Traffic_history, 'label': Traffic_output}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "     \n",
    "class LSTM_SpeedPrediction_database_output_multi_sensor(Dataset):\n",
    "    def __init__(self, Traffic, Weather, data, transform=None, look_back = 6):\n",
    "\n",
    "        self.Traffic = Traffic\n",
    "        self.Weather = Weather\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        self.look_back = look_back\n",
    "#         printdb_trans_1 = Dataset_NVIDIA_1(annotation_list,frame_list,transform = PerspectiveTransform(MAP_FILE, world_origin, pixel_ratio, PMAT_FILE))(self.__len__())\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        assert idx < self.__len__(),'idx out of dataset index boundary'\n",
    "        Traffic_today = self.Traffic[idx,:,:-1,:]\n",
    "        Weather_today = self.Weather[idx,:,:-1,:]\n",
    "        Traffic_history = 0 \n",
    "        Traffic_output = self.Traffic[idx,:,1:,0]\n",
    "        \n",
    "        dayofweek = self.data.loc[idx]['dayofweek']\n",
    "        data_sub = self.data[:idx]\n",
    "        sameday_in_history = data_sub.index[data_sub['dayofweek'] == dayofweek].tolist()\n",
    "        \n",
    "#         Traffic_history = np.full((self.look_back,)+self.Traffic.shape[1:], np.nan)\n",
    "        \n",
    "        Traffic_history = [self.Traffic[idx,:,:,:]+np.random.randn(15,1440,3) for _ in range(self.look_back)]\n",
    "        Traffic_history = np.stack(Traffic_history,0)\n",
    "        \n",
    "#         print(len(sameday_in_history))\n",
    "        sameday_in_near_history = sameday_in_history[-self.look_back:]\n",
    "#         print(len(sameday_in_near_history))\n",
    "        \n",
    "        for i in range(len(sameday_in_near_history)):\n",
    "            Traffic_history[self.look_back-len(sameday_in_near_history)+i] = self.Traffic[sameday_in_near_history[i]]\n",
    "        \n",
    "        sample = {'traffic': torch.Tensor(Traffic_today),'weather': torch.Tensor(Weather_today), 'history': torch.Tensor(Traffic_history), 'label': torch.Tensor(Traffic_output)} # \n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = LSTM_SpeedPrediction_database_output_multi_sensor(Traffic, Weather, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 431  # test: [417,420,431]\n",
    "print(data.loc[idx])\n",
    "\n",
    "def vis_one_day_traffic_speed(speed):\n",
    "    plt.pcolor(speed,cmap=my_cmap, vmin=20, vmax=70)\n",
    "    \n",
    "# vis_one_day_traffic_speed(db[257]['label'])\n",
    "print(type(db[idx]['traffic']))\n",
    "vis_one_day_traffic_speed(db[idx]['traffic'][:,:,0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_test = data.loc[[417,420,431]]\n",
    "# data_test = data_test.reset_index()\n",
    "\n",
    "# data_train = data\n",
    "# data_train = data_train.drop([417,420,431])\n",
    "# data_train = data_train.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataloader: load sample from database in batch\n",
    "\n",
    "## batch added one more dimension to the left\n",
    "\n",
    "## in case of multi-output and batch = 10, each sample:\n",
    "\n",
    "## {'traffic':(10,15,1439,3),'weather':(10,15,1439,10),'history':(10,6,15,1439,3),'label':(10,15,1439)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data[:-30]\n",
    "data_train = data_train.reset_index()\n",
    "\n",
    "data_val = data[-61:]\n",
    "data_val = data_val.reset_index()\n",
    "\n",
    "dataset = {}\n",
    "dataset['train'] = LSTM_SpeedPrediction_database_output_multi_sensor(Traffic[:-30], Weather[:-30], data_train)\n",
    "dataset['val'] = LSTM_SpeedPrediction_database_output_multi_sensor(Traffic[-61:], Weather[-61:], data_val)\n",
    "\n",
    "dataset_sizes = {x: len(dataset[x]) for x in ['train', 'val']}\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(dataset[x], batch_size=1000,\n",
    "                                             shuffle=True, num_workers=4)\n",
    "              for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sample in dataloaders['train']:\n",
    "#     print('traffic: ',sample['traffic'].shape)\n",
    "#     print('weather: ',sample['weather'].shape)\n",
    "#     print('history: ',sample['history'].shape)\n",
    "#     print('label: ',sample['label'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''https://github.com/wkelongws/iemgrid'''\n",
    "'''Weather: 0:tmpc 1:dwpc 2:smps 3:drct 4:vsby 5:roadtmpc 6:srad 7:snwd 8:pcpn '''\n",
    "\n",
    "Traffic_max = []\n",
    "Traffic_min = []\n",
    "Weather_max = []\n",
    "Weather_min = []\n",
    "\n",
    "for i in range(Traffic.shape[-1]):\n",
    "#     print(i,'max: ',np.max(Traffic[:,:,:,i]))\n",
    "    Traffic_max.append(np.max(Traffic[:,:,:,i]))\n",
    "#     print(i,'mean: ',np.mean(Traffic[:,:,:,i]))\n",
    "#     print(i,'min: ',np.min(Traffic[:,:,:,i]))\n",
    "    Traffic_min.append(np.min(Traffic[:,:,:,i]))\n",
    "for i in range(Weather.shape[-1]):\n",
    "#     print(i,'max: ',np.max(Weather[:,:,:,i]))\n",
    "    Weather_max.append(np.max(Weather[:,:,:,i]))\n",
    "#     print(i,'mean: ',np.mean(Weather[:,:,:,i]))\n",
    "#     print(i,'min: ',np.min(Weather[:,:,:,i]))\n",
    "    Weather_min.append(np.min(Weather[:,:,:,i]))\n",
    "    \n",
    "Min_Max = (Traffic_max,Traffic_min,Weather_max,Weather_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Min_Max[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Prediction Net\n",
    "## Normalization on the fly. Normalize all numbers ~ (0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Short_Term_Speed_Pred_Net(nn.Module):\n",
    "    def __init__(self, filter_height = 5, output_single_sensor = False, \\\n",
    "                 hidden_dim = 16,use_gpu = True, use_volume_and_occup = False, \\\n",
    "                 use_weather = False, use_history = True, input_feature = 'cnn', Min_Max = Min_Max):\n",
    "        super(LSTM_Short_Term_Speed_Pred_Net, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.out_dim = 15\n",
    "        \n",
    "        self.input_dim_today = self.out_dim\n",
    "        self.input_dim_history = self.out_dim\n",
    "        if input_feature == 'cnn':\n",
    "            self.input_dim_today = self.out_dim - filter_height + 1\n",
    "            self.input_dim_history = self.out_dim - filter_height + 1\n",
    "#         else:\n",
    "#             self.input_dim_today = self.out_dim\n",
    "#             self.input_dim_history = self.out_dim\n",
    "        \n",
    "        if output_single_sensor:\n",
    "            self.out_dim = out_dim = 1\n",
    "        self.use_gpu = use_gpu\n",
    "        \n",
    "        self.use_volume_and_occup = use_volume_and_occup\n",
    "        self.use_weather = use_weather\n",
    "        self.use_history = use_history\n",
    "#         assert input_feature in ['raw','linear','cnn'], 'wrong choice for input_feature'\n",
    "        assert input_feature == 'cnn', 'currently only allow cnn as input feature extractor'\n",
    "        self.input_feature = input_feature # ['raw','linear','CNN']\n",
    "        \n",
    "        '''for on-fly normalization'''\n",
    "        self.Traffic_max = Min_Max[0]\n",
    "        self.Traffic_min = Min_Max[1]\n",
    "        self.Weather_max = Min_Max[2]\n",
    "        self.Weather_min = Min_Max[3]\n",
    "        \n",
    "        if self.input_feature == 'cnn':\n",
    "            self.CNN_for_history = nn.Conv2d(1, 1, (filter_height, 1), stride=(1, 1))\n",
    "            self.CNN_feature_extract = nn.Conv2d(1, 1, (filter_height, 1), stride=(1, 1))\n",
    "            if self.use_volume_and_occup:\n",
    "                self.CNN_feature_extract = nn.Conv2d(1, 1, (filter_height, 3), stride=(1, 1))\n",
    "            if self.use_weather:\n",
    "                self.CNN_feature_extract = nn.Conv2d(1, 1, (filter_height, 12), stride=(1, 1))\n",
    "        self.CNN_today_and_history_output_combine = nn.Conv2d(1, 1, (1, 2), stride=(1, 1))\n",
    "        self.lstm_today = nn.LSTMCell(self.input_dim_today, self.hidden_dim)\n",
    "        self.lstm_history = nn.LSTMCell(self.input_dim_history, self.hidden_dim)\n",
    "        self.linear_out = nn.Linear(self.hidden_dim, self.out_dim)\n",
    "        \n",
    "    def init_hidden(self,input_sample):\n",
    "        \n",
    "        batch_size = input_sample['traffic'].shape[0]\n",
    "        if self.use_gpu:\n",
    "            return Variable(torch.zeros(batch_size, self.hidden_dim).cuda(), requires_grad=False)\n",
    "        return Variable(torch.zeros(batch_size, self.hidden_dim), requires_grad=False)\n",
    "\n",
    "    def LSTM_update(self,input_t,hidden_state_today,cell_state_today):\n",
    "        input_t = input_t.squeeze(3).squeeze(1)\n",
    "        hidden_state_today, cell_state_today = self.lstm_today(input_t, (hidden_state_today, cell_state_today))\n",
    "        hidden_state_output = hidden_state_today\n",
    "        return hidden_state_today, cell_state_today, hidden_state_output\n",
    " \n",
    "    def LSTM_update_with_history(self,input_t,input_history,hidden_state_today,cell_state_today,hidden_state_history,cell_state_history):\n",
    "        input_t = input_t.squeeze(3).squeeze(1)\n",
    "        hidden_state_today, cell_state_today = self.lstm_today(input_t, (hidden_state_today, cell_state_today))\n",
    "        if len(input_history)>0:\n",
    "            for input_t_history in input_history:\n",
    "                input_h = input_t_history.squeeze(3).squeeze(1)\n",
    "                hidden_state_history, cell_state_history = self.lstm_history(input_h, (hidden_state_history, cell_state_history))\n",
    "\n",
    "            h_combine = torch.stack([hidden_state_today,hidden_state_history],2)\n",
    "            h_combine = h_combine.unsqueeze(1)\n",
    "            hidden_state_output = self.CNN_today_and_history_output_combine(h_combine)\n",
    "            hidden_state_output = hidden_state_output.squeeze(3).squeeze(1)\n",
    "            return hidden_state_today, cell_state_today, hidden_state_output\n",
    "        else:\n",
    "            hidden_state_output = hidden_state_today\n",
    "            return hidden_state_today, cell_state_today, hidden_state_output\n",
    "        \n",
    "\n",
    "    def forward(self, input_sample, future = 0, use_gpu = True):\n",
    "        \n",
    "        '''reconstruct data from sample'''\n",
    "        traffic = input_sample['traffic']\n",
    "        weather = input_sample['weather']\n",
    "        history = input_sample['history']\n",
    "        label = input_sample['label']\n",
    "#         print('traffic: ',traffic.shape)\n",
    "#         print('weather: ',weather.shape)\n",
    "#         print('history: ',history.shape)\n",
    "#         print('label: ',label.shape)\n",
    "#         print()\n",
    "#         print(np.sum(np.isnan(np.array(traffic))))\n",
    "#         print(np.sum(np.isnan(np.array(weather))))\n",
    "#         print(np.sum(np.isnan(np.array(history))))\n",
    "        \n",
    "        '''normalize'''\n",
    "#         label = (label - self.Traffic_min[0])/(self.Traffic_max[0]-self.Traffic_min[0])\n",
    "        for i in range(traffic.shape[-1]):\n",
    "            traffic[:,:,:,i] = (traffic[:,:,:,i]-self.Traffic_min[i])/(self.Traffic_max[i]-self.Traffic_min[i])\n",
    "            history[:,:,:,:,i] = (history[:,:,:,:,i]-self.Traffic_min[i])/(self.Traffic_max[i]-self.Traffic_min[i])\n",
    "        for i in range(weather.shape[-1]):\n",
    "            weather[:,:,:,i] = (weather[:,:,:,i]-self.Weather_min[i])/(self.Weather_max[i]-self.Weather_min[i])\n",
    "        \n",
    "#         print()\n",
    "#         print(np.sum(np.isnan(np.array(traffic))))\n",
    "#         print(np.sum(np.isnan(np.array(weather))))\n",
    "#         print(np.sum(np.isnan(np.array(history))))\n",
    "        '''re-arrange inputs'''\n",
    "        input_today = traffic[:,:,:,0:1]\n",
    "        \n",
    "        if self.use_volume_and_occup:\n",
    "            input_today = traffic\n",
    "            \n",
    "        if self.use_weather:\n",
    "            input_today = torch.cat([traffic,weather],3)\n",
    "            \n",
    "        input_history = history[:,:,:,:,0:1]\n",
    "            \n",
    "        if use_gpu:\n",
    "            input_today = Variable(input_today.cuda())\n",
    "            input_history = Variable(input_history.cuda())\n",
    "            label = Variable(label.cuda())\n",
    "        else:\n",
    "            input_today = Variable(input_today)\n",
    "            input_history = Variable(input_history)\n",
    "            label = Variable(label)\n",
    "        \n",
    "#         print()\n",
    "#         print(np.sum(np.isnan(np.array(input_today.data))))\n",
    "#         print(np.sum(np.isnan(np.array(input_history.data))))\n",
    "        \n",
    "        '''feedforward'''\n",
    "        \n",
    "        outputs = []\n",
    "        hidden_state_today = self.init_hidden(input_sample)\n",
    "        cell_state_today = self.init_hidden(input_sample)\n",
    "        hidden_state_history = self.init_hidden(input_sample)\n",
    "        cell_state_history = self.init_hidden(input_sample)\n",
    "#         print(input_today.data.shape[2])\n",
    "        for i in range(input_today.data.shape[2]):\n",
    "            \n",
    "            input_t = input_today[:,:,i:i+1,:]\n",
    "            input_t = input_t.permute(0,2,1,3)\n",
    "            \n",
    "            input_t_history = []\n",
    "#             print(input_history.shape[1])\n",
    "            for k in range(input_history.shape[1]):\n",
    "                if not np.isnan(np.array(input_history[0,k,0,0,0].data))[0]:\n",
    "                    input_t_history.append(input_history[:,k,:,i+1:i+2,:].permute(0,2,1,3))\n",
    "#             print(len(input_t_history))\n",
    "\n",
    "            if self.input_feature == 'cnn':\n",
    "                input_t = self.CNN_feature_extract(input_t)\n",
    "                input_t_history = [self.CNN_for_history(x) for x in input_t_history]\n",
    "\n",
    "            if self.use_history:\n",
    "                hidden_state_today, cell_state_today, hidden_state_output = self.LSTM_update_with_history(input_t,input_t_history,hidden_state_today,cell_state_today,hidden_state_history,cell_state_history)\n",
    "            else:\n",
    "                hidden_state_today, cell_state_today, hidden_state_output = self.LSTM_update(input_t,hidden_state_today,cell_state_today)\n",
    "            \n",
    "            output = nn.functional.sigmoid(self.linear_out(hidden_state_output))\n",
    "            output = output*(self.Traffic_max[0]-self.Traffic_min[0])+self.Traffic_min[0] \n",
    "            outputs += [output]\n",
    "        outputs = torch.stack(outputs, 2)\n",
    "\n",
    "        return outputs, label\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "# use_gpu = False\n",
    "\n",
    "model = LSTM_Short_Term_Speed_Pred_Net(filter_height = 5, output_single_sensor = False, \\\n",
    "                 hidden_dim = 16,use_gpu = True, use_volume_and_occup = True, \\\n",
    "                 use_weather = True, use_history = True, input_feature = 'cnn')\n",
    "\n",
    "if use_gpu:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred,label = model(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(pred),pred.shape)\n",
    "print(type(label),label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(np.isnan(np.array(label.data))))\n",
    "print(np.sum(np.isnan(np.array(pred.data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(np.array(pred.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "906570/(15*1439*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for sample in dataloaders['train']:\n",
    "    cnt += 1\n",
    "    if cnt == 1:\n",
    "        s = sample\n",
    "    b = sample['traffic']\n",
    "    c = sample['weather']\n",
    "    d = sample['history']\n",
    "    e = sample['label']\n",
    "    print(type(sample['traffic']))\n",
    "    print('traffic: ',sample['traffic'].shape)\n",
    "    print('weather: ',sample['weather'].shape)\n",
    "    print('history: ',sample['history'].shape)\n",
    "    print('label: ',sample['label'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def customized_loss(preds, targets):\n",
    "    \n",
    "#     preds = preds.view(preds.data.shape[0]*preds.data.shape[1],-1)\n",
    "#     targets = targets.view(targets.data.shape[0]*targets.data.shape[1],-1)\n",
    "    \n",
    "#     pdist = nn.PairwiseDistance(p=2)\n",
    "    \n",
    "#     output = pdist(preds, targets)\n",
    "\n",
    "#     loss = sum(output)/output.data.shape[0]\n",
    "    \n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "# use_gpu = False\n",
    "\n",
    "model = LSTM_Short_Term_Speed_Pred_Net(filter_height = 5, output_single_sensor = False, \\\n",
    "                 hidden_dim = 16,use_gpu = True, use_volume_and_occup = True, \\\n",
    "                 use_weather = True, use_history = True, input_feature = 'cnn')\n",
    "\n",
    "if use_gpu:\n",
    "    model = model.cuda()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "# exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LSTM_ShortTerm_SP_Net(model,dataloaders, criterion, optimizer, dataset_sizes, num_epochs=100):\n",
    "    since = time.time()\n",
    "\n",
    "#     timeSince(since)\n",
    "#     model.load_state_dict(torch.load('Best_LSTM_Weights_1st'))\n",
    "#     print()\n",
    "#     print('keep training from previous \"Best_LSTM_Weights_1st\"')\n",
    "#     print()\n",
    "\n",
    "#     vanillaPlus_compromised_dict = torch.load('Best_LSTM_Weights_vanillaPlus')\n",
    "#     vanillaPlus_compromised_dict['linear_out.weight'] = torch.randn(model.state_dict()['linear_out.weight'].shape).cuda()\n",
    "#     vanillaPlus_compromised_dict['linear_out.bias'] = torch.randn(model.state_dict()['linear_out.bias'].shape).cuda()\n",
    "#     model.load_state_dict(vanillaPlus_compromised_dict)\n",
    "#     print()\n",
    "#     print('weights in the output layer are initialized from normal distribution')\n",
    "#     print('and other model weights are loaded from \"Best_LSTM_Weights_vanillaPlus_compromised')\n",
    "#     print('keep training from previous \"Best_LSTM_Weights_vanillaPlus_compromised\"')\n",
    "#     print()\n",
    "\n",
    "    best_model_wts = model.state_dict()\n",
    "    best_loss = 100000\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train(True)  # Set model to training mode\n",
    "            else:\n",
    "                model.train(False)  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for sample in dataloaders[phase]:\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                model.zero_grad()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Also, we need to clear out the hidden state of the LSTM,\n",
    "                # detaching it from its history on the last instance.\n",
    "#                 model.hidden = model.init_hidden()\n",
    "                \n",
    "                # forward\n",
    "                pred,label = model(sample)\n",
    "                \n",
    "                # loss function\n",
    "#                 loss = customized_loss(pred,label)\n",
    "                loss = criterion(pred,label)\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.data[0]\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} total elapsed time: {}'.format(phase, epoch_loss, timeSince(since)))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = model.state_dict()\n",
    "#                 torch.save(best_model_wts, 'Best_LSTM_Weights')\n",
    "                print('Weights saved to \"Best_LSTM_Weights\"')\n",
    "\n",
    "        print()\n",
    "\n",
    "    print('Training complete in {}'.format(timeSince(since)))\n",
    "    print('Best val loss: {:4f}'.format(best_loss))\n",
    "    \n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    torch.save(model, 'Model_with_Best_LSTM_Weights')\n",
    "    print('Model saved to \"Model_with_Best_LSTM_Weights\"')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = train_LSTM_ShortTerm_SP_Net(model,dataloaders, criterion, optimizer, dataset_sizes, num_epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
